
services:
  #livekit:
  #  image: livekit/livekit-server:latest
  #  command: --dev --node-ip livekit
  #  ports:
  #    - "7880:7880"
  #    - "7881:7881"
  #    - "7882:7882/udp"
  #    - "50000-50100:50000-50100/udp" # media ports
  #  environment:
  #    LIVEKIT_KEYS: "devkey: secret"
  #    LIVEKIT_NODE_IP: "192.168.255.14" 

  # Old Coqui TTS - commented out
  # coqui-tts:
  #   container_name: coqui-tts
  #   image: ghcr.io/coqui-ai/tts-cpu:latest
  #   ports:
  #     - "5002:5002"
  #   environment:
  #     - MODEL_NAME=tts_models/en/ljspeech/tacotron2-DDC
  #   volumes:
  #     - ./coqui-tts_models:/root/.local/share/tts
  #   entrypoint: ["python3", "TTS/server/server.py"]
  #   #command: ["--model_name", "tts_models/en/ljspeech/tacotron2-DDC"]
  #   command: ["--model_name", "tts_models/en/ljspeech/glow-tts"]

  # Coqui TTS server with neural models
  coqui-tts:
    container_name: coqui-tts
    build:
      context: .
      dockerfile: Dockerfile.tts
    ports:
      - "5000:5000"
    environment:
      - TTS_MODEL=tts_models/en/ljspeech/tacotron2-DDC  # Can be changed to other models
    volumes:
      - ./coqui-tts:/app/coqui-tts  # Mount code for development
      - ./coqui-tts_models:/root/.local/share/tts  # Mount model cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  postgres:
    container_name: postgres
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: litellm
      POSTGRES_PASSWORD: password123
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "4000:4000"
    environment:
      - LITELLM_MASTER_KEY=sk-1234
      - DATABASE_URL=postgresql://litellm:password123@postgres:5432/litellm
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "1"]
    depends_on:
      - postgres
      - ollama

  agent-app:
    container_name: agent-app
    build:
      context: .
      dockerfile: Dockerfile.dev
    volumes:
      - ./src:/app/src
      - ./agent-models:/root/.cache/huggingface/hub/
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env.local
    command: ["python", "-m", "src.agent", "dev"]
    depends_on:
      - ollama
      - litellm
      - coqui-tts
    deploy:
      resources:
        limits:
          memory: 3G  # Increase memory limit for larger models
        reservations:
          memory: 1G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    #depends_on:
    #  - livekit

volumes:
  ollama_data:
  postgres_data:
